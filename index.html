<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Subash Khanal</title>

    <meta name="author" content="Subash Khanal">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Subash Khanal
                </p>
                <p>I am a PhD candidate in Computer Science at the <a href="https://wustl.edu/">Washington University in St. Louis </a>, working in <a href="https://mvrl.cse.wustl.edu/">Multimodal Vision Research Laboratory</a> led by <a href="https://engineering.wustl.edu/faculty/Nathan-Jacobs.html">Dr. Nathan Jacobs</a>.
                </p>
                <p>
                  I have a MS in Electrical Engineering from the <a href="https://www.uky.edu/">University of Kentucky</a> in <a href="https://en.wikipedia.org/wiki/Lexington,_Kentucky"> Lexington</a>.
                  During my masters, I worked with <a href="http://johnson.engr.uky.edu">Dr. Michael T. Johnson</a> focusing on speech recognition and signal processing.
                </p>
                <p style="text-align:center">
                  <a href="mailto:subash.khanal.cs@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Subash_Khanal_Resume_April2024.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=G72g3R0AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/subash-khanal/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/subash-khanal/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/photo2_circular.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo2_circular.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in using deep learning to solve various Computer Vision (CV) problems while learning from multimodal data. 
                  Currently, I am focusing on developing CV models having geospatial understanding of sounds around the world. 
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Publications</h2>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:98%;border:10px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <table style="width:98%;border:10px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2309.10667.pdf" id="MCG_journal">
                  <span class="papertitle">Learning Tri-modal Embeddings for Zero-Shot Soundscape Mapping</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Sastry Srikumar, Dhakal Aayush and Jacobs Nathan
                <br>
                <em>BMVC</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2309.10667.pdf">arxiv</a> /
                <a href="data/GeoCLAP_supplementary.pdf">supplementary</a> /
                <a href="data/geoclapBMVC2023.bib">bibtex</a> /
                <a href="https://github.com/mvrl/geoclap">code</a>
                <p></p>
                <p>We learn a tri-modal embedding space between audio, text and overhead imagery. This enables us to create soundscape maps over any geographic region, using either audio or textual queries.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2307.15904.pdf" id="MCG_journal">
                  <span class="papertitle">Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images</span>
                </a>
                <br>
                Dhakal Aayush, Ahmad Adeel, <strong>Khanal Subash</strong>, Sastry Srikumar, Kerner Hannah and Jacobs Nathan
                <br>
                <em>CVPRW (EarthVision)</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2307.15904.pdf">arxiv</a> /
                <a href="data/sat2cap2023.bib">bibtex</a> /
                <a href="https://github.com/mvrl/GeoClip/">code</a>
                <p></p>
                <p>We train a contrastive learning framework, Sat2Cap on a novel large scale dataset. This enables us to create maps using free-form textual descriptions.</p>
              </td>
            </tr>
          
            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/abs/2404.06637" id="MCG_journal">
                  <span class="papertitle">GeoSynth: Contextually-Aware High-Resolution Satellite Image Synthesis</span>
                </a>
                <br>
                Sastry Srikumar, <strong>Khanal Subash</strong>, Dhakal Aayush, and Jacobs Nathan
                <br>
                <em>CVPRW (EarthVision)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2404.06637">arxiv</a> /
                <a href="data/geosynth.bib">bibtex</a> /
                <a href="https://github.com/mvrl/GeoSynth">code</a>
                <p></p>
                <p>
                This work presents GeoSynth, a diffusion-based model for synthesizing satellite images with global style and image-driven layout control.</p>
              </td>
            </tr>
          
            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2312.08334.pdf" id="MCG_journal">
                  <span class="papertitle">LD-SDM: Language-Driven Hierarchical Species Distribution Modeling</span>
                </a>
                <br>
                Sastry Srikumar, Xin Xing, Dhakal Aayush, <strong>Khanal Subash</strong>, Ahmad Adeel, and Jacobs Nathan
                <br>
                <em>preprint</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2312.08334.pdf">arxiv</a> /
                <a href="data/ldsdm.bib">bibtex</a> 
                <p></p>
                <p>
                 We introduced a novel approach for species distribution modeling that uses a large-language model to generate a representation of species. This provides flexibility to generate range maps at different levels of the taxonomic hierarchy and for unseen species.</p>
              </td>
            </tr>
          
          <tr>
            <td width="100%" valign="middle">
              <a href="https://arxiv.org/pdf/2310.19168v1" id="MCG_journal">
                <span class="papertitle">BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping</span>
              </a>
              <br>
              Sastry Srikumar, <strong>Khanal Subash</strong>, Di Huang, Dhakal Aayush and Jacobs Nathan
              <br>
              <em>WACV</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2310.19168v1">arxiv</a> /
              <a href="data/birdsat2024.bib">bibtex</a> /
              <a href="https://github.com/mvrl/BirdSAT">code</a>
              <p></p>
              <p>
              This work presents a flexible framework, with vector embedding and metric learning variants, that supports both species distribution mapping with fine-grained visual classification.</p>
            </td>
          </tr>
          

          <tr>
            <td width="100%" valign="middle">
              <a href="https://arxiv.org/abs/2404.11720" id="MCG_journal">
                <span class="papertitle">GeoBind: Binding text, image, and audio through satellite images.</span>
              </a>
              <br>
              Dhakal Aayush, Khanal Subash, Sastry Srikumar, Ahmad Adeel, Jacobs Nathan
              <br>
              <em>IGARSS </em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2404.11720">arxiv</a> /
              <a href="data/geobind.bib">bibtex</a> 
              <p></p>
              <p>
              This work presents a general framework that can be used to create an embedding space with any number of modalities by using satellite images as the binding element.</p>
            </td>
          </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2206.14841.pdf" id="MCG_journal">
                  <span class="papertitle">Causality for inherently explainable transformers: CAT-XPLAIN</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Brodie Benjamin, Xing Xin, Lin Ai-Ling and Jacobs Nathan
                <br>
                <em>CVPR Workshop</em>, 2022
                <br>
                <a href="https://arxiv.org/pdf/2206.14841.pdf">arxiv</a> /
                <a href="data/catxplain2022.bib">bibtex</a> /
                <a href="https://github.com/mvrl/CAT-XPLAIN">code</a>
                <p></p>
                <p>Add an extra special token (explainable token) into Vision Transformer (ViT), and train it to select the most important patches in the input image.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9761584" id="MCG_journal">
                  <span class="papertitle">Advit: Vision transformer on multi-modality pet images for alzheimer disease diagnosis</span>
                </a>
                <br>
                Xing Xin, Liang Gongbo, Zhang Yu, <strong>Khanal Subash</strong>, Lin Ai-Ling and Jacobs Nathan.
                <br>
                <em>ISBI</em>, 2022
                <br>
                <a href="https://ieeexplore.ieee.org/document/9761584">paper</a> /
                <a href="data/advit2022.bib">bibtex</a> 
                <p></p>
                <p> Training ViT on 3D-to-2D converted multi-modal PET images achieves better Alzheimer's disease prediction.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9669730" id="MCG_journal">
                  <span class="papertitle">Alzheimer's Disease Classification Using Genetic Data</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Chen Jin, Jacobs Nathan and Lin Ai-Ling
                <br>
                <em>BIBM Workshop</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/document/9669730">paper</a> /
                <a href="data/genetics2021.bib">bibtex</a> /
                <a href="https://github.com/mvrl/ADNI_Genetics">code</a>
                <p></p>
                <p> Machine learning on different types of genetic data helps to identify candidate genes for Alzheimer's disease progression.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9554405" id="MCG_journal">
                  <span class="papertitle">Hierarchical Probabilistic Embeddings for Multi-View Image Classification</span>
                </a>
                <br>
                Brodie Benjamin, <strong>Khanal Subash</strong>, Rafique Muhammad Usman, Greenwell Connor and Jacobs Nathan
                <br>
                <em>IGARSS</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/document/9554405">paper</a> /
                <a href="data/probEmb2021.bib">bibtex</a> 
                <p></p>
                <p> Learning a hierarchical, probabilistic embedding space allows one to achieve uncertainty estimate of feature distributions coming from sources with variable bands of information.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9383574" id="MCG_journal">
                  <span class="papertitle">Articulatory Comparison of L1 and L2 Speech for Mispronunciation Diagnosis</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Johnson Michael T. and Bozorg Narjess
                <br>
                <em>SLT</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/document/9383574">paper</a> /
                <a href="data/slt2021.bib">bibtex</a> 
                <p></p>
                <p> This paper compares the difference in articulatory patterns between native (L1) and non-native (L2) Mandarin speakers of English, for the purpose of providing an understanding of mispronunciation behaviors of L2 learners.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9587408" id="MCG_journal">
                  <span class="papertitle">Mispronunciation Detection and Diagnosis for Mandarin Accented English Speech</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Johnson Michael T., Soleymanpour Mohammad and Bozorg Narjes
                <br>
                <em>SpeD</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/document/9587408">paper</a> /
                <a href="data/sped2021.bib">bibtex</a> 
                <p></p>
                <p>Articulatory features improve the performance of Automatic Speech Recognition (ASR) based Mispronunciation Detection and Diagnosis (MDD) systems.</p>
              </td>
            </tr>
            
            <tr>
              <td width="100%" valign="middle">
                <a href="https://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1164&context=ece_etds" id="MCG_journal">
                  <span class="papertitle">Mispronunciation Detection and Diagnosis in Mandarin Accented English Speech</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>
                <br>
                <em>Theses and Dissertations--Electrical and Computer Engineering</em>, 2020
                <br>
                <a href="https://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1164&context=ece_etds"> Thesis</a> /
                <a href="data/thesis.bib">bibtex</a>
                <p></p>
                <p> The focus of this work was to analyse articulatory patterns of mispronunciation and design of ASR based MDD system.</p>
              </td>
            </tr>

            
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  This website is modified from source code of <a href="https://github.com/jonbarron/jonbarron_website">John Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
