<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Subash Khanal</title>

    <meta name="author" content="Subash Khanal">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Subash Khanal
                </p>
                <p>I am a PhD candidate in Computer Science at the <a href="https://wustl.edu/">Washington University in St. Louis </a>, working in <a href="https://mvrl.cse.wustl.edu/">Multimodal Vision Research Laboratory</a> led by <a href="https://engineering.wustl.edu/faculty/Nathan-Jacobs.html">Dr. Nathan Jacobs</a>.
                </p>
                <p>
                I have a MS in Electrical Engineering from the <a href="https://www.uky.edu/">University of Kentucky</a> in <a href="https://en.wikipedia.org/wiki/Lexington,_Kentucky"> Lexington</a>.
                During my masters, I worked with <a href="http://johnson.engr.uky.edu">Dr. Michael T. Johnson</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:subash.khanal.cs@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Subash_Khanal_Resume_latest.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=G72g3R0AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/subash-khanal/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/subash-khanal/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/photo2_circular.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo2_circular.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                 My research focuses on multimodal self-supervised learning and generative AI. I have experience building deep learning models that integrate satellite imagery, audio, and text, as well as curating large-scale datasets to support research across various applications in geospatial AI.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Publications</h2>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:98%;border:10px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <table style="width:98%;border:10px;border-spacing:10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <!-- Publications ordered by year (2025, 2024, 2023, etc.) -->
            
            <!-- 2025 Publications -->

            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2505.13777" id="MCG_journal">
                  <span class="papertitle">Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Sastry Srikumar, Dhakal Aayush, Ahmad Adeel and Jacobs Nathan
                <br>
                <em>preprint</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2505.13777">arxiv</a>/
                <a href="data/Sat2Sound_demo.mp4">demo video</a>/
                <a href="data/sat2sound.bib">bibtex</a> 
                <p></p>
                <p>
                A state-of-the-art soundscape mapping framework that leverages a Vision-Language Model (VLM) to enrich the semantic understanding of a locationâ€™s soundscape, learns a shared codebook for fine-grained alignment, and enables retrieval-based, location-conditioned soundscape generation.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2407.09672" id="MCG_journal">
                  <span class="papertitle">Mixed-View Panorama Synthesis using Geospatially Guided Diffusion</span>
                </a>
                <br>
                Xiong Zhexiao, Xing Xin, Workman Scott, <strong>Khanal Subash</strong>, Jacobs Nathan
                <br>
                <em>Accepted to TMLR</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2407.09672">arxiv</a> /
                <a href="data/mixed.bib">bibtex</a> 
                <p></p>
                <p>
                This work introduces the task of mixed-view panorama synthesis, where the goal is to synthesize a novel panorama given a small set of input panoramas and a satellite image of the area.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2502.19781" id="MCG_journal">
                  <span class="papertitle">RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings</span>
                </a>
                <br>
                Dhakal Aayush, Sastry Srikumar, <strong> Khanal Subash </strong>, Ahmad Adeel, Xing Eric and Jacobs Nathan
                <br>
                <em>CVPR</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2502.19781">arxiv</a> /
                <a href="data/range2025.bib">bibtex</a>
                <p></p>
                <p>We propose a novel retrieval-augmented strategy for multi-resolution geo-embeddings, called RANGE. Our method is based on the intuition that the visual features of a location can be estimated by aggregating visual features from multiple similar-looking locations.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2411.00683" id="MCG_journal">
                  <span class="papertitle">TaxaBind: A Unified Embedding Space for Ecological Applications</span>
                </a>
                <br>
                Sastry Srikumar, <strong>Khanal Subash</strong>, Dhakal Aayush, Ahmad Adeel and Jacobs Nathan
                <br>
                <em>WACV</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2411.00683">arxiv</a> /
                <a href="data/taxabind.bib">bibtex</a> /
                <a href="https://github.com/mvrl/TaxaBind">code</a> /
                <a href="https://vishu26.github.io/taxabind/index.html">project page</a>
                <p></p>
                <p>TaxaBind is a suite of multimodal models useful for downstream ecological tasks covering six modalities: ground-level image, geographic location, satellite image, text, audio, and environmental features.</p>
              </td>
            </tr>
            

            <!-- 2024 Publications -->
            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2408.07050" id="MCG_journal">
                  <span class="papertitle">PSM: Learning Probabilistic Embeddings for Multi-scale Zero-Shot Soundscape Mapping</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Xing Eric, Sastry Srikumar, Dhakal Aayush,  Xiong Zhexiao, Ahmad Adeel and Jacobs Nathan
                <br>
                <em>ACM Multimedia</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2408.07050">arxiv</a> /
                <a href="data/PSM.bib">bibtex</a> /
                <a href="https://github.com/mvrl/PSM">code</a> /
                <a href="PSM/index.html">project page</a>
                <p></p>
                <p>We develop a probabilistic, multi-scale, and metadata-aware embedding space that connects audio, text, and overhead imagery. This enables the creation of dynamic, multi-scale soundscape maps for any geographic region, along with uncertainty estimates for the mapping.</p>
              </td>
            </tr>
            
            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2307.15904.pdf" id="MCG_journal">
                  <span class="papertitle">Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images</span>
                </a>
                <br>
                Dhakal Aayush, Ahmad Adeel, <strong>Khanal Subash</strong>, Sastry Srikumar, Kerner Hannah and Jacobs Nathan
                <br>
                <em>CVPRW (EarthVision), 2024, <strong>Best Paper Award</strong> </em>
                <br>
                <a href="https://arxiv.org/pdf/2307.15904.pdf">arxiv</a> /
                <a href="data/sat2cap2023.bib">bibtex</a> /
                <a href="https://github.com/mvrl/GeoClip/">code</a>
                <p></p>
                <p>We train a contrastive learning framework, Sat2Cap on a novel large scale dataset. This enables us to create maps using free-form textual descriptions.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/abs/2404.06637" id="MCG_journal">
                  <span class="papertitle">GeoSynth: Contextually-Aware High-Resolution Satellite Image Synthesis</span>
                </a>
                <br>
                Sastry Srikumar, <strong>Khanal Subash</strong>, Dhakal Aayush, and Jacobs Nathan
                <br>
                <em>CVPRW (EarthVision)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2404.06637">arxiv</a> /
                <a href="data/geosynth.bib">bibtex</a> /
                <a href="https://github.com/mvrl/GeoSynth">code</a> /
                <a href="https://vishu26.github.io/geosynth/index.html">project page</a>
                <p></p>
                <p>
                This work presents GeoSynth, a diffusion-based model for synthesizing satellite images with global style and image-driven layout control.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/abs/2404.11720" id="MCG_journal">
                  <span class="papertitle">GeoBind: Binding text, image, and audio through satellite images.</span>
                </a>
                <br>
                Dhakal Aayush, <strong>Khanal Subash</strong>, Sastry Srikumar, Ahmad Adeel, Jacobs Nathan
                <br>
                <em>IGARSS </em>, 2024, <strong>Oral Presentation</strong> </em>
                <br>
                <a href="https://arxiv.org/abs/2404.11720">arxiv</a> /
                <a href="data/geobind.bib">bibtex</a> 
                <p></p>
                <p>
                This work presents a general framework that can be used to create an embedding space with any number of modalities by using satellite images as the binding element.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2312.08334.pdf" id="MCG_journal">
                  <span class="papertitle">LD-SDM: Language-Driven Hierarchical Species Distribution Modeling</span>
                </a>
                <br>
                Sastry Srikumar, Xin Xing, Dhakal Aayush, <strong>Khanal Subash</strong>, Ahmad Adeel, and Jacobs Nathan
                <br>
                <em>preprint</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2312.08334.pdf">arxiv</a> /
                <a href="data/ldsdm.bib">bibtex</a> 
                <p></p>
                <p>
                 We introduced a novel approach for species distribution modeling that uses a large-language model to generate a representation of species. This provides flexibility to generate range maps at different levels of the taxonomic hierarchy and for unseen species.</p>
              </td>
            </tr>
            
            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2310.19168v1" id="MCG_journal">
                  <span class="papertitle">BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping</span>
                </a>
                <br>
                Sastry Srikumar, <strong>Khanal Subash</strong>, Di Huang, Dhakal Aayush and Jacobs Nathan
                <br>
                <em>WACV</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2310.19168v1">arxiv</a> /
                <a href="data/birdsat2024.bib">bibtex</a> /
                <a href="https://github.com/mvrl/BirdSAT">code</a>
                <p></p>
                <p>
                This work presents a flexible framework, with vector embedding and metric learning variants, that supports both species distribution mapping with fine-grained visual classification.</p>
              </td>
            </tr>

            <!-- 2023 Publications -->
            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2309.10667.pdf" id="MCG_journal">
                  <span class="papertitle">Learning Tri-modal Embeddings for Zero-Shot Soundscape Mapping</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Sastry Srikumar, Dhakal Aayush and Jacobs Nathan
                <br>
                <em>BMVC</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2309.10667.pdf">arxiv</a> /
                <a href="data/GeoCLAP_supplementary.pdf">supplementary</a> /
                <a href="data/geoclapBMVC2023.bib">bibtex</a> /
                <a href="https://github.com/mvrl/geoclap">code</a>
                <p></p>
                <p>We learn a tri-modal embedding space between audio, text and overhead imagery. This enables us to create soundscape maps over any geographic region, using either audio or textual queries.</p>
              </td>
            </tr>

            <!-- 2022 Publications -->
            <tr>
              <td width="100%" valign="middle">
                <a href="https://arxiv.org/pdf/2206.14841.pdf" id="MCG_journal">
                  <span class="papertitle">Causality for inherently explainable transformers: CAT-XPLAIN</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Brodie Benjamin, Xing Xin, Lin Ai-Ling and Jacobs Nathan
                <br>
                <em>CVPR Workshop</em>, 2022
                <br>
                <a href="https://arxiv.org/pdf/2206.14841.pdf">arxiv</a> /
                <a href="data/catxplain2022.bib">bibtex</a> /
                <a href="https://github.com/mvrl/CAT-XPLAIN">code</a>
                <p></p>
                <p>Add an extra special token (explainable token) into Vision Transformer (ViT), and train it to select the most important patches in the input image.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9761584" id="MCG_journal">
                  <span class="papertitle">Advit: Vision transformer on multi-modality pet images for alzheimer disease diagnosis</span>
                </a>
                <br>
                Xing Xin, Liang Gongbo, Zhang Yu, <strong>Khanal Subash</strong>, Lin Ai-Ling and Jacobs Nathan.
                <br>
                <em>ISBI</em>, 2022
                <br>
                <a href="https://ieeexplore.ieee.org/document/9761584">paper</a> /
                <a href="data/advit2022.bib">bibtex</a> 
                <p></p>
                <p> Training ViT on 3D-to-2D converted multi-modal PET images achieves better Alzheimer's disease prediction.</p>
              </td>
            </tr>

            <!-- 2021 Publications -->
            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9669730" id="MCG_journal">
                  <span class="papertitle">Alzheimer's Disease Classification Using Genetic Data</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Chen Jin, Jacobs Nathan and Lin Ai-Ling
                <br>
                <em>BIBM Workshop</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/document/9669730">paper</a> /
                <a href="data/genetics2021.bib">bibtex</a> /
                <a href="https://github.com/mvrl/ADNI_Genetics">code</a>
                <p></p>
                <p> Machine learning on different types of genetic data helps to identify candidate genes for Alzheimer's disease progression.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9554405" id="MCG_journal">
                  <span class="papertitle">Hierarchical Probabilistic Embeddings for Multi-View Image Classification</span>
                </a>
                <br>
                Brodie Benjamin, <strong>Khanal Subash</strong>, Rafique Muhammad Usman, Greenwell Connor and Jacobs Nathan
                <br>
                <em>IGARSS</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/document/9554405">paper</a> /
                <a href="data/probEmb2021.bib">bibtex</a> 
                <p></p>
                <p> Learning a hierarchical, probabilistic embedding space allows one to achieve uncertainty estimate of feature distributions coming from sources with variable bands of information.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9383574" id="MCG_journal">
                  <span class="papertitle">Articulatory Comparison of L1 and L2 Speech for Mispronunciation Diagnosis</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Johnson Michael T. and Bozorg Narjess
                <br>
                <em>SLT</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/document/9383574">paper</a> /
                <a href="data/slt2021.bib">bibtex</a> 
                <p></p>
                <p> This paper compares the difference in articulatory patterns between native (L1) and non-native (L2) Mandarin speakers of English, for the purpose of providing an understanding of mispronunciation behaviors of L2 learners.</p>
              </td>
            </tr>

            <tr>
              <td width="100%" valign="middle">
                <a href="https://ieeexplore.ieee.org/document/9587408" id="MCG_journal">
                  <span class="papertitle">Mispronunciation Detection and Diagnosis for Mandarin Accented English Speech</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>, Johnson Michael T., Soleymanpour Mohammad and Bozorg Narjes
                <br>
                <em>SpeD</em>, 2021
                <br>
                <a href="https://ieeexplore.ieee.org/document/9587408">paper</a> /
                <a href="data/sped2021.bib">bibtex</a> 
                <p></p>
                <p>Articulatory features improve the performance of Automatic Speech Recognition (ASR) based Mispronunciation Detection and Diagnosis (MDD) systems.</p>
              </td>
            </tr>

            <!-- 2020 Publications -->
            <tr>
              <td width="100%" valign="middle">
                <a href="https://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1164&context=ece_etds" id="MCG_journal">
                  <span class="papertitle">Mispronunciation Detection and Diagnosis in Mandarin Accented English Speech</span>
                </a>
                <br>
                <strong>Khanal Subash</strong>
                <br>
                <em>Theses and Dissertations--Electrical and Computer Engineering</em>, 2020
                <br>
                <a href="https://uknowledge.uky.edu/cgi/viewcontent.cgi?article=1164&context=ece_etds"> Thesis</a> /
                <a href="data/thesis.bib">bibtex</a>
                <p></p>
                <p> The focus of this work was to analyse articulatory patterns of mispronunciation and design of ASR based MDD system.</p>
              </td>
            </tr>
            
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  This website is modified from source code of <a href="https://github.com/jonbarron/jonbarron_website">John Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>